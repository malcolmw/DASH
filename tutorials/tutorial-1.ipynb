{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fc81a1a-1862-4c68-bb39-36290b14aee9",
   "metadata": {},
   "source": [
    "# Tutorial-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1940ec-4594-498a-a6c1-fbecdc1dd7fb",
   "metadata": {},
   "source": [
    "This second exercise demonstrates how to create a set of HDF5eis files with multi-dimensional data and link them to a single master file. To begin, let's import some necessary packages for this tutorial and create a directory where we can write some data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dba57533-4e5a-49e7-8a9a-dcef3e1a334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import io\n",
    "import pathlib\n",
    "\n",
    "# Third-party imports\n",
    "import hdf5eis\n",
    "import numpy as np\n",
    "import obspy.clients.fdsn\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "OUTPUT_DIR = pathlib.Path(\"/home/malcolmw/scratch/hdf5eis_tutorial\")\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2116a08c-4c86-4f17-a638-e675f29a4dc8",
   "metadata": {},
   "source": [
    "First let's simulate writing ten seconds of DAS data to ten separate files. Each file will contain one second of raw DAS data sampled from 1000 channels at 4000 samples per second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6ffeea-ea32-4c22-ab69-36b2e0ec227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.rand(1000, 4000)\n",
    "start_time = pd.to_datetime(\"2022-01-01T00:00:00Z\")\n",
    "sampling_rate = 4000\n",
    "tag = \"DAS/raw\"\n",
    "\n",
    "for i in range(10):\n",
    "    with hdf5eis.File(OUTPUT_DIR.joinpath(f\"file-{i:02d}.hdf5\"), mode=\"w\") as out_file:\n",
    "        out_file.timeseries.add(\n",
    "            data,\n",
    "            start_time,\n",
    "            sampling_rate,\n",
    "            tag=tag\n",
    "        )\n",
    "    start_time = start_time + pd.to_timedelta(data.shape[1]/sampling_rate, unit=\"S\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d94b0b-4c22-4849-bef3-a0d23f97e873",
   "metadata": {},
   "source": [
    "Now we can link the individual files to one master file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61bd2f12-089e-41a1-836f-bc2bcefbc8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_files = sorted(OUTPUT_DIR.iterdir())\n",
    "                   \n",
    "with hdf5eis.File(OUTPUT_DIR.joinpath(f\"master.hdf5\"), mode=\"w\") as out_file:\n",
    "    for src_file in src_files:\n",
    "        out_file.timeseries.link_tag(src_file, \"DAS/raw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c55ad0e-8d2a-416b-b2c7-1d2637bf2a2f",
   "metadata": {},
   "source": [
    "All ten files are now accessible from the master file, and data can be seamlessly read across file boundaries. Let's read one second of data spanning a file boundary from channels 256 through 511."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08a9481-9a6a-4cff-a4ed-c7a5c2036d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DAS/raw': [<hdf5eis.gather.Gather at 0x7ff1b926b370>]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = \"2022-01-01T00:00:01.5Z\"\n",
    "end_time   = \"2022-01-01T00:00:02.5Z\"\n",
    "\n",
    "with hdf5eis.File(OUTPUT_DIR.joinpath(f\"master.hdf5\"), mode=\"r\") as in_file:\n",
    "    gather = in_file.timeseries[\"DAS/raw\", 256: 512, start_time: end_time]\n",
    "    \n",
    "gather"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4637642-b0a4-4532-9767-ef30d1dedc1f",
   "metadata": {},
   "source": [
    "It may by inefficient or otherwise intractable to write the data all at once. For example, one may wish to convert a 2D array of 3C geophone data from miniSEED files to HDF5eis format. In cases such as this, one can create and empty HDF5eis DataSet and write data to it in a piece-wise fashion.\n",
    "\n",
    "Consider a 2D array of 3C geophones with 16 rows and 32 columns. The below simulates conversion from channel-day miniSEED files to HDF5eis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f82f4f2-8489-4171-981c-de386682d5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_miniSEED(station, channel, start_time, end_time):\n",
    "    \"\"\"\n",
    "    This function simply returns some random data, but in reality would need to\n",
    "    retrieve the actual data from the appropriate miniSEED file.\n",
    "    \"\"\"\n",
    "    return (np.random.rand(8640000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31b0fc69-0fef-42af-8251-1883b1f55ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding data for station R1515.\r"
     ]
    }
   ],
   "source": [
    "with hdf5eis.File(OUTPUT_DIR.joinpath(\"Array2D.hdf5\"), mode=\"w\") as out_file:\n",
    "    ds = out_file.timeseries.create_dataset(\n",
    "        (16, 16, 3, 8640000), # One day of data from a 16 by 16 array of 3C geophones sampled at 100 sps.\n",
    "        \"2022-01-01T00:00:00Z\",\n",
    "        100,\n",
    "        tag=\"geophones/2D\"\n",
    "    )\n",
    "    for irow in range(16):\n",
    "        for icol in range(16):\n",
    "            station = f\"R{irow:02d}{icol:02d}\"\n",
    "            print(f\"Adding data for station {station}.\", end=\"\\r\")\n",
    "            for icomp in range(3):\n",
    "                ds[irow, icol, icomp, :] = read_miniSEED(\n",
    "                    station, \n",
    "                    icomp, \n",
    "                    \"2022-01-01T00:00:00Z\",\n",
    "                    \"2022-01-02T00:00:00Z\"\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5779602-ffe3-41fc-808e-ef441dd8f4de",
   "metadata": {},
   "source": [
    "We can link this to the same master file as the DAS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62dbad83-54f5-40d3-956a-be2da7808620",
   "metadata": {},
   "outputs": [],
   "source": [
    "with hdf5eis.File(OUTPUT_DIR.joinpath(\"master.hdf5\"), mode=\"a\") as out_file:\n",
    "    out_file.timeseries.link_tag(\n",
    "        OUTPUT_DIR.joinpath(\"Array2D.hdf5\"), \n",
    "        \"geophones/2D\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9761c8-2726-4973-a8c4-313d50c8f453",
   "metadata": {},
   "source": [
    "And we can slice across any of the storage dimensions. For example, let's slice on hour of data from rows 8 through 11, columns 2 through 9, and channels 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "029c11a7-03a4-402e-920d-f6e8984ecb20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 2, 360001)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = \"2022-01-01T06:09:12.03Z\"\n",
    "end_time   = \"2022-01-01T07:09:12.03Z\"\n",
    "with hdf5eis.File(OUTPUT_DIR.joinpath(\"master.hdf5\"), mode=\"r\") as in_file:\n",
    "    gather = in_file.timeseries[\"geophones/2D\", 8:12, 2: 10, :2, start_time: end_time]\n",
    "    \n",
    "gather[\"geophones/2D\"][0].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb238f-43e5-41dd-ba4b-6c47501d69eb",
   "metadata": {},
   "source": [
    "Or we can extract one second of data from all channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4d4ff67-0e81-4e7f-aac2-e5583736c2db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16, 3, 101)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = \"2022-01-01T06:09:12.03Z\"\n",
    "end_time   = \"2022-01-01T06:09:13.03Z\"\n",
    "with hdf5eis.File(OUTPUT_DIR.joinpath(\"master.hdf5\"), mode=\"r\") as in_file:\n",
    "    gather = in_file.timeseries[\"geophones/2D\", ..., start_time: end_time]\n",
    "    \n",
    "gather[\"geophones/2D\"][0].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e970582-3aa5-4308-9d89-30f3d4ea8c3a",
   "metadata": {},
   "source": [
    "Those are the basics of ingesting and linking multi-dimensional data. For a more thorough example of ingesting data, see the `ingest_ZG_data` script for ingesting miniSEED data from the Sage Brush Flats Nodal Experiment (Ben-Zion et al., 2015).\n",
    "\n",
    "Ben-Zion, Y., Vernon, F. L., Ozakin, Y., Zigone, D., Ross, Z. E., Meng, H., White, M., Reyes, J., Hollis, D., & Barklage, M. (2015). Basic data features  and results from a spatially dense seismic array on the San Jacinto fault zone. _Geophysical Journal International, 202_(1), 370â€“380. https://doi.org/10. 1093/gji/ggv142"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:devel]",
   "language": "python",
   "name": "conda-env-devel-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
